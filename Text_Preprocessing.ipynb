{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb846fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=11, micro=4, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0616a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=9ce82d8fc7150920b5d29e4fbe0e0820b99939ca67bfc6f27ad8a7217b7dd550\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\40\\b3\\0f\\a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea79fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import wget\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45f842",
   "metadata": {},
   "source": [
    "# 2.1 Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc200e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casing_text -> artificial intelligence(ai), sometimes called machine intelligence, we learning it\n"
     ]
    }
   ],
   "source": [
    "text = \"Artificial intelligence(AI), sometimes called machine intelligence, we learning it\"\n",
    "casing_text = text.lower()\n",
    "print(\"casing_text ->\" , casing_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe132357",
   "metadata": {},
   "source": [
    "# 2.2 Removal of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97289de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT_TO_REMOVE -> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "remove_punct_text -> Artificial intelligenceAI sometimes called machine intelligence we learning it\n"
     ]
    }
   ],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "print(\"PUNCT_TO_REMOVE ->\" , PUNCT_TO_REMOVE)\n",
    "\n",
    "text = \"Artificial intelligence(AI), sometimes called machine intelligence, we learning it\"\n",
    "\n",
    "remove_punct_text = text.translate(str.maketrans('','',PUNCT_TO_REMOVE))\n",
    "print(\"remove_punct_text ->\" , remove_punct_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe671d89",
   "metadata": {},
   "source": [
    "# 2.3 Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bf1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOPWORDS -> i,me,my,myself,we,our,ours,ourselves,you,you're,you've,you'll,you'd,your,yours,yourself,yourselves,he,him,his,himself,she,she's,her,hers,herself,it,it's,its,itself,they,them,their,theirs,themselves,what,which,who,whom,this,that,that'll,these,those,am,is,are,was,were,be,been,being,have,has,had,having,do,does,did,doing,a,an,the,and,but,if,or,because,as,until,while,of,at,by,for,with,about,against,between,into,through,during,before,after,above,below,to,from,up,down,in,out,on,off,over,under,again,further,then,once,here,there,when,where,why,how,all,any,both,each,few,more,most,other,some,such,no,nor,not,only,own,same,so,than,too,very,s,t,can,will,just,don,don't,should,should've,now,d,ll,m,o,re,ve,y,ain,aren,aren't,couldn,couldn't,didn,didn't,doesn,doesn't,hadn,hadn't,hasn,hasn't,haven,haven't,isn,isn't,ma,mightn,mightn't,mustn,mustn't,needn,needn't,shan,shan't,shouldn,shouldn't,wasn,wasn't,weren,weren't,won,won't,wouldn,wouldn't\n",
      "remove_stopword_text -> Artificial intelligence(AI), sometimes called machine intelligence, learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "print(\"STOPWORDS ->\" , ','.join(STOPWORDS))\n",
    "\n",
    "text = \"Artificial intelligence(AI), sometimes called machine intelligence, we learning it\"\n",
    "\n",
    "remove_stopword_text = [word for word in text.split() if word not in STOPWORDS]\n",
    "print(\"remove_stopword_text ->\" , \" \".join(remove_stopword_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3301ff2",
   "metadata": {},
   "source": [
    "# 2.4 Removal of Frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dad595f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter -> Counter({'intelligence': 3, 'artificial': 2, '(aI),': 1, 'sometimes': 1, 'called': 1, 'machine': 1, 'was': 1, 'founded': 1, 'as': 1, 'an': 1, 'academic': 1, 'discipline': 1, 'in': 1, '1955': 1}) \n",
      "\n",
      "FREQWORDS -> {'(aI),', 'intelligence', 'artificial'} \n",
      "\n",
      "remove_freq_text -> [['sometimes', 'called', 'machine'], ['was', 'founded', 'as', 'an', 'academic', 'discipline', 'in', '1955']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "num = 3\n",
    "text = [\"artificial intelligence (aI), sometimes called machine intelligence\".split(),\n",
    "        \"artificial intelligence was founded as an academic discipline in 1955\".split()]\n",
    "\n",
    "cnt = Counter([word for sent in text for word in sent])\n",
    "print('Counter ->' , cnt , '\\n')\n",
    "FREQWORDS = set(w for (w,wc) in cnt.most_common(num))\n",
    "print(\"FREQWORDS ->\" , FREQWORDS , '\\n')\n",
    "\n",
    "remove_freq_text = [[word for word in sent if word not in FREQWORDS] for sent in text]\n",
    "print(\"remove_freq_text ->\" , remove_freq_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aac99f",
   "metadata": {},
   "source": [
    "# 2.5 Removal of Rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f577ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAREWORDS -> ['as', 'an', 'academic', 'discipline', 'in'] \n",
      "\n",
      "remove_rare_text -> [['artificial', 'intelligence', '(aI),', 'sometimes', 'called', 'machine', 'intelligence'], ['artificial', 'intelligence', 'was', 'founded', '1955']]\n"
     ]
    }
   ],
   "source": [
    "num = 5\n",
    "text = [\"artificial intelligence (aI), sometimes called machine intelligence\".split(),\n",
    "        \"artificial intelligence was founded as an academic discipline in 1955\".split()]\n",
    "\n",
    "cnt = Counter([word for sent in text for word in sent])\n",
    "\n",
    "RAREWORDS = [w for (w,wc) in cnt.most_common()[-num-1:-1]]\n",
    "print(\"RAREWORDS ->\" , RAREWORDS , '\\n')\n",
    "\n",
    "remove_rare_text = [[word for word in sent if word not in RAREWORDS] for sent in text]\n",
    "print(\"remove_rare_text ->\" , remove_rare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56eaa45",
   "metadata": {},
   "source": [
    "# 2.6 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5ec2f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmer -> <PorterStemmer> \n",
      "\n",
      "stem_text ->  artifici intelligence(ai), sometim call machin intelligence, we learn it\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(\"stemmer ->\" , stemmer , '\\n')\n",
    "\n",
    "text = \"Artificial intelligence(AI), sometimes called machine intelligence, we learning it\"\n",
    "stem_text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "print(\"stem_text -> \" , stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33cf37ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem_text ->  soochow university, found in 1900 in suzhou, wa the first western-styl univers in china. after the gover of the republ of china move to taiwan.\n"
     ]
    }
   ],
   "source": [
    "text = \"Soochow University, founded in 1900 in Suzhou, was the first western-style university in China. After the goverment of the Republic of China move to Taiwan.\"\n",
    "stem_text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "print(\"stem_text -> \" , stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf6c10",
   "metadata": {},
   "source": [
    "# 2.7 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd98ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_text -> artificial intelligence wa founded a an academic discipline in 1955\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"artificial intelligence was founded as an academic discipline in 1955\"\n",
    "lemma_text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "print(\"lemma_text ->\" , lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da752072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_text -> Soochow University, founded in 1900 in Suzhou, wa the first western-style university in China. After the goverment of the Republic of China move to Taiwan.\n"
     ]
    }
   ],
   "source": [
    "text = \"Soochow University, founded in 1900 in Suzhou, was the first western-style university in China. After the goverment of the Republic of China move to Taiwan.\"\n",
    "lemma_text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "print(\"lemma_text ->\" , lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6fc553",
   "metadata": {},
   "source": [
    "# 2.8 Removal of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7121d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_url_text -> artificial intelligence was founded as an academic discipline in 1955 at \n"
     ]
    }
   ],
   "source": [
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "text = \"artificial intelligence was founded as an academic discipline in 1955 at https://en.wikipedia.org/wiki/Artificial_intelligence.\"\n",
    "\n",
    "remove_url_text = url_pattern.sub(r'' , text)\n",
    "print(\"remove_url_text ->\" , remove_url_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213b786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
